{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10266417,"sourceType":"datasetVersion","datasetId":6351441}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"google/vit-base-patch16-224-in21k\", \"facebook/deit-base-patch16-224\", \"microsoft/swin-base-patch4-window7-224\", \"microsoft/beit-base-patch16-224-pt22k-ft22k\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport torch\n\ndef seed_everything(seed=42):\n    \"\"\"\n    Python, Numpy, PyTorch ve cuDNN kütüphanelerini\n    aynı 'seed' ile sabitler.\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # Çoklu GPU varsa\n    \n    # PyTorch determinism ayarları\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n    # (Opsiyonel) Reproducibility log\n    print(f\"[INFO] Global seed set to {seed}\")\n\nseed_everything()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T03:17:51.902974Z","iopub.execute_input":"2024-12-22T03:17:51.903191Z","iopub.status.idle":"2024-12-22T03:17:54.890190Z","shell.execute_reply.started":"2024-12-22T03:17:51.903163Z","shell.execute_reply":"2024-12-22T03:17:54.889287Z"}},"outputs":[{"name":"stdout","text":"[INFO] Global seed set to 42\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"##############################################\n# EĞİTİM KODU (Z-Score + Haversine + AMP + Optuna + CosineAnnealingLR + 2 GPU)\n# Ek: Daha Fazla Print & TQDM (Debug Logging)\n##############################################\n\nimport os\nimport math\nimport time\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms\n\n# transformers kütüphanesi\nfrom transformers import AutoConfig, AutoModel\nfrom tqdm import tqdm\n\n# Automatic Mixed Precision\nfrom torch.cuda.amp import autocast, GradScaler\n\n# Optuna\nimport optuna\n\n#######################\n# 1) Z-SCORE İSTATİSTİKLERİ\n#######################\nLAT_MEAN = 41.105763\nLAT_STD  = 0.002358\nLON_MEAN = 29.025191\nLON_STD  = 0.004205\n\ndef zscore_lat_lon(lat, lon):\n    lat_norm = (lat - LAT_MEAN) / LAT_STD\n    lon_norm = (lon - LON_MEAN) / LON_STD\n    return lat_norm, lon_norm\n\ndef inverse_zscore_lat_lon(lat_norm, lon_norm):\n    lat_real = lat_norm * LAT_STD + LAT_MEAN\n    lon_real = lon_norm * LON_STD + LON_MEAN\n    return lat_real, lon_real\n\n#######################\n# 2) HAVERSINE MESAFE HESABI\n#######################\ndef haversine_distance_tensor(lat_pred, lon_pred, lat_true, lon_true):\n    R = 6371_000.0  # Dünya yarıçapı (metre)\n    lat_pred_rad = lat_pred * (math.pi / 180.0)\n    lon_pred_rad = lon_pred * (math.pi / 180.0)\n    lat_true_rad = lat_true * (math.pi / 180.0)\n    lon_true_rad = lon_true * (math.pi / 180.0)\n\n    dlat = lat_true_rad - lat_pred_rad\n    dlon = lon_true_rad - lon_pred_rad\n\n    a = torch.sin(dlat / 2)**2 + torch.cos(lat_pred_rad) * torch.cos(lat_true_rad) * torch.sin(dlon / 2)**2\n    c = 2 * torch.atan2(torch.sqrt(a), torch.sqrt(1 - a))\n\n    distances = R * c\n    return distances.mean()\n\n#######################\n# 3) CUSTOM LOSS\n#######################\nmse_criterion = nn.MSELoss()\n\ndef combined_loss(outputs, targets, alpha=0.95):\n    # 1) MSE (normalleştirilmiş lat/lon)\n    mse = mse_criterion(outputs, targets)\n\n    # 2) Haversine\n    lat_pred_norm = outputs[:, 0]\n    lon_pred_norm = outputs[:, 1]\n    lat_true_norm = targets[:, 0]\n    lon_true_norm = targets[:, 1]\n\n    lat_pred_deg, lon_pred_deg = inverse_zscore_lat_lon(lat_pred_norm, lon_pred_norm)\n    lat_true_deg, lon_true_deg = inverse_zscore_lat_lon(lat_true_norm, lon_true_norm)\n\n    haversine_m = haversine_distance_tensor(lat_pred_deg, lon_pred_deg, lat_true_deg, lon_true_deg)\n    total_loss = alpha * mse + (1 - alpha) * (haversine_m / 1000.0)\n    return total_loss\n\n#########################\n# 4) DATASET TANIMI\n#########################\nclass CampusDataset(Dataset):\n    def __init__(self, dataframe, img_dir, transform=None, use_zscore=True):\n        self.dataframe = dataframe\n        self.img_dir = img_dir\n        self.transform = transform\n        self.use_zscore = use_zscore\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        # Buraya debug print konulabilir ama çok sık olur => yavaşlatır\n        row = self.dataframe.iloc[idx]\n        image_path = os.path.join(self.img_dir, row[\"filename\"])\n        lat = float(row[\"latitude\"])\n        lon = float(row[\"longitude\"])\n        \n        image = Image.open(image_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        \n        if self.use_zscore:\n            lat, lon = zscore_lat_lon(lat, lon)\n        \n        target = torch.tensor([lat, lon], dtype=torch.float32)\n        return image, target\n\n#########################\n# 5) MODEL TANIMI\n#########################\nclass TransformerRegressor(nn.Module):\n    \"\"\"\n    Tek bir model sınıfı ile DeiT, Swin, BEiT, ViT vs. AutoModel ile yükleyebilirsiniz.\n    \"\"\"\n    def __init__(self, model_name=\"microsoft/swin-base-patch4-window7-224\", output_dim=2):\n        super().__init__()\n        self.config = AutoConfig.from_pretrained(model_name)\n        self.backbone = AutoModel.from_pretrained(model_name, config=self.config)\n\n        hidden_size = self.config.hidden_size\n        self.head = nn.Sequential(\n            nn.Linear(hidden_size, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, output_dim)\n        )\n\n    def forward(self, x):\n        outputs = self.backbone(x)\n        if hasattr(outputs, 'last_hidden_state'):\n            cls_emb = outputs.last_hidden_state[:, 0, :]\n        else:\n            cls_emb = outputs.pooler_output\n        preds = self.head(cls_emb)\n        return preds\n\n#########################\n# 6) OPTUNA OBJECTIVE\n#########################\ndef objective(trial, train_dataset, device):\n    print(\"\\n[Optuna] => Yeni deneme (trial) başlıyor...\")\n    alpha = trial.suggest_float(\"alpha\", 0.90, 0.99)\n    lr = trial.suggest_loguniform(\"lr\", 1e-5, 5e-4)\n\n    model_name = \"microsoft/swin-base-patch4-window7-224\"\n\n    n_total = len(train_dataset)\n    n_val = int(0.2 * n_total)\n    n_train = n_total - n_val\n\n    print(f\"[Optuna] => Dataset toplam: {n_total}, Train: {n_train}, Val: {n_val}\")\n    train_ds, val_ds = random_split(train_dataset, [n_train, n_val])\n\n    # Burada num_workers=4 => Dağılımlar\n    # pin_memory=True => bazen hız, bazen yavaşlama. Duruma göre değiştirip test edebilirsiniz.\n    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n    val_loader   = DataLoader(val_ds,   batch_size=16, shuffle=False, num_workers=4, pin_memory=True)\n\n    model = TransformerRegressor(model_name=model_name, output_dim=2).to(device)\n    if torch.cuda.device_count() > 1:\n        print(f\"[Optuna] => {torch.cuda.device_count()} GPU var, DataParallel aktif.\")\n        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n\n    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n\n    # CosineAnnealingLR (3 epoch => T_max=3)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=3, eta_min=1e-6)\n\n    scaler = GradScaler()\n\n    EPOCHS = 3\n    for epoch in range(EPOCHS):\n        print(f\"[Optuna] => Epoch {epoch+1}/{EPOCHS} başlıyor...\")\n        model.train()\n        t0 = time.time()\n        running_loss = 0.0\n\n        # Eklemek istediğimiz: train_loader üzerinde tqdm ile batch'leri gösterelim\n        with tqdm(total=len(train_loader), desc=f\"OptunaTrain E{epoch+1}\", unit=\"batch\") as pbar:\n            for batch_idx, (images, targets) in enumerate(train_loader):\n                images, targets = images.to(device), targets.to(device)\n\n                optimizer.zero_grad()\n                with autocast():\n                    outputs = model(images)\n                    loss = combined_loss(outputs, targets, alpha=alpha)\n\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n\n                running_loss += loss.item()\n                pbar.set_postfix({\"loss\": f\"{running_loss/(batch_idx+1):.4f}\"})\n                pbar.update(1)\n\n        scheduler.step()\n        print(f\"[Optuna] => Epoch {epoch+1} bitti, süresi: {time.time()-t0:.2f} sn\")\n\n    # Validation\n    print(\"[Optuna] => Validation aşaması başlıyor...\")\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad(), autocast():\n        with tqdm(total=len(val_loader), desc=\"Val\", unit=\"batch\") as pbar2:\n            for batch_idx, (images, targets) in enumerate(val_loader):\n                images, targets = images.to(device), targets.to(device)\n                preds = model(images)\n                loss = combined_loss(preds, targets, alpha=alpha)\n                val_loss += loss.item()\n                pbar2.update(1)\n\n    val_loss /= len(val_loader)\n    print(f\"[Optuna] => Validation loss: {val_loss:.4f}\")\n    return val_loss\n\n#########################\n# 7) ANA EĞİTİM FONKSİYONU\n#########################\nprint(\"=== Başlangıç ===\")\nCSV_FILE  = \"/kaggle/input/hulyais/train.csv\"\nIMG_DIR   = \"/kaggle/input/hulyais/train\"\n\nprint(\"CSV okunuyor...\")\ndf = pd.read_csv(CSV_FILE, sep=';')\nprint(f\"CSV satır sayısı: {len(df)}\")\n\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                         [0.229, 0.224, 0.225])\n])\n\nprint(\"Dataset oluşturuluyor...\")\ndataset = CampusDataset(\n    dataframe=df,\n    img_dir=IMG_DIR,\n    transform=train_transform,\n    use_zscore=True\n)\nprint(f\"Dataset length: {len(dataset)}\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}, GPU Sayısı: {torch.cuda.device_count()}\")\n\n#########################\n# 7.1) OPTUNA TUNING\n#########################\ndef optuna_objective(trial):\n    return objective(trial, dataset, device)\n\nprint(\"=== Optuna Study oluşturuluyor ===\")\nstudy = optuna.create_study(direction=\"minimize\")\nprint(\"=== Optuna Study optimize başlıyor ===\")\nstudy.optimize(optuna_objective, n_trials=2)  # n_trials=2 test amaçlı, artırabilirsiniz.\n\nprint(\"Optuna tamamlandı! Best params:\")\nprint(study.best_params)\nprint(\"Best value (val_loss):\", study.best_value)\n\nalpha_best = study.best_params[\"alpha\"]\nlr_best = study.best_params[\"lr\"]\n\n#########################\n# 7.2) NİHAİ EĞİTİM\n#########################\nprint(\"\\n=== Nihai eğitim başlıyor ===\")\ntrain_loader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n\nfinal_model_name = \"microsoft/swin-base-patch4-window7-224\"\nmodel = TransformerRegressor(model_name=final_model_name, output_dim=2).to(device)\nif torch.cuda.device_count() > 1:\n    print(f\"Nihai eğitim: {torch.cuda.device_count()} GPU var, DataParallel aktif.\")\n    model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n\noptimizer = optim.AdamW(model.parameters(), lr=lr_best, weight_decay=1e-4)\n\n# CosineAnnealingLR (5 epoch => T_max=5)\nFINAL_EPOCHS = 25\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=FINAL_EPOCHS, eta_min=1e-6)\n\nscaler = GradScaler()\n\nfor epoch in range(FINAL_EPOCHS):\n    print(f\"[Nihai] => Epoch {epoch+1}/{FINAL_EPOCHS} başlıyor...\")\n    model.train()\n    running_loss = 0.0\n    t_start = time.time()\n\n    with tqdm(total=len(train_loader), desc=f\"NihaiTrain E{epoch+1}\", unit=\"batch\") as pbar:\n        for batch_idx, (images, targets) in enumerate(train_loader):\n            images, targets = images.to(device), targets.to(device)\n\n            optimizer.zero_grad()\n            with autocast():\n                outputs = model(images)\n                loss = combined_loss(outputs, targets, alpha=alpha_best)\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            running_loss += loss.item()\n            pbar.set_postfix({\"loss\": f\"{running_loss/(batch_idx+1):.4f}\"})\n            pbar.update(1)\n\n    scheduler.step()\n    epoch_loss = running_loss / len(train_loader)\n    print(f\"[Nihai] => Epoch {epoch+1} bitti. Loss: {epoch_loss:.4f}, Süre: {time.time()-t_start:.2f} sn, LR: {scheduler.get_last_lr()}\")\n\nprint(\"Nihai eğitim tamamlandı.\")\ntorch.save(model.state_dict(), \"/kaggle/working/final_model.pth\")\nprint(\"Model kaydedildi: /kaggle/working/final_model.pth\")\n\n\n\n\"\"\"\nfinal 8 için 0.21 verdi\n\n\"\"\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-22T03:18:10.076980Z","iopub.execute_input":"2024-12-22T03:18:10.077302Z","iopub.status.idle":"2024-12-22T06:43:33.300217Z","shell.execute_reply.started":"2024-12-22T03:18:10.077269Z","shell.execute_reply":"2024-12-22T06:43:33.299138Z"}},"outputs":[{"name":"stderr","text":"[I 2024-12-22 03:18:12,831] A new study created in memory with name: no-name-53176996-53bf-4cfe-ae76-fbbaf63dd725\n","output_type":"stream"},{"name":"stdout","text":"=== Başlangıç ===\nCSV okunuyor...\nCSV satır sayısı: 4375\nDataset oluşturuluyor...\nDataset length: 4375\nDevice: cuda, GPU Sayısı: 2\n=== Optuna Study oluşturuluyor ===\n=== Optuna Study optimize başlıyor ===\n\n[Optuna] => Yeni deneme (trial) başlıyor...\n[Optuna] => Dataset toplam: 4375, Train: 3500, Val: 875\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-2-b26102738f1a>:153: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform(\"lr\", 1e-5, 5e-4)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/71.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d452428dfde741b4bf4e4f6eac26afc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/352M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01bfd763a11a40d3b56ec76395df05e9"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-2-b26102738f1a>:179: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n","output_type":"stream"},{"name":"stdout","text":"[Optuna] => 2 GPU var, DataParallel aktif.\n[Optuna] => Epoch 1/3 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"OptunaTrain E1:   0%|          | 0/219 [00:00<?, ?batch/s]<ipython-input-2-b26102738f1a>:194: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nOptunaTrain E1: 100%|██████████| 219/219 [05:21<00:00,  1.47s/batch, loss=0.9279]\n","output_type":"stream"},{"name":"stdout","text":"[Optuna] => Epoch 1 bitti, süresi: 321.43 sn\n[Optuna] => Epoch 2/3 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"OptunaTrain E2: 100%|██████████| 219/219 [05:27<00:00,  1.49s/batch, loss=0.6727]\n","output_type":"stream"},{"name":"stdout","text":"[Optuna] => Epoch 2 bitti, süresi: 327.20 sn\n[Optuna] => Epoch 3/3 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"OptunaTrain E3: 100%|██████████| 219/219 [05:27<00:00,  1.49s/batch, loss=0.4072]\n<ipython-input-2-b26102738f1a>:213: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.no_grad(), autocast():\n","output_type":"stream"},{"name":"stdout","text":"[Optuna] => Epoch 3 bitti, süresi: 327.26 sn\n[Optuna] => Validation aşaması başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"Val: 100%|██████████| 55/55 [01:25<00:00,  1.56s/batch]\n[I 2024-12-22 03:35:58,036] Trial 0 finished with value: 0.4008280569856817 and parameters: {'alpha': 0.9283114805391769, 'lr': 0.00030238576049720445}. Best is trial 0 with value: 0.4008280569856817.\n<ipython-input-2-b26102738f1a>:153: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform(\"lr\", 1e-5, 5e-4)\n","output_type":"stream"},{"name":"stdout","text":"[Optuna] => Validation loss: 0.4008\n\n[Optuna] => Yeni deneme (trial) başlıyor...\n[Optuna] => Dataset toplam: 4375, Train: 3500, Val: 875\n[Optuna] => 2 GPU var, DataParallel aktif.\n[Optuna] => Epoch 1/3 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"OptunaTrain E1: 100%|██████████| 219/219 [05:32<00:00,  1.52s/batch, loss=0.7171]\n","output_type":"stream"},{"name":"stdout","text":"[Optuna] => Epoch 1 bitti, süresi: 332.59 sn\n[Optuna] => Epoch 2/3 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"OptunaTrain E2: 100%|██████████| 219/219 [05:26<00:00,  1.49s/batch, loss=0.3472]\n","output_type":"stream"},{"name":"stdout","text":"[Optuna] => Epoch 2 bitti, süresi: 326.02 sn\n[Optuna] => Epoch 3/3 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"OptunaTrain E3: 100%|██████████| 219/219 [05:26<00:00,  1.49s/batch, loss=0.2104]\n","output_type":"stream"},{"name":"stdout","text":"[Optuna] => Epoch 3 bitti, süresi: 326.79 sn\n[Optuna] => Validation aşaması başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"Val: 100%|██████████| 55/55 [01:18<00:00,  1.42s/batch]\n[I 2024-12-22 03:53:42,105] Trial 1 finished with value: 0.24859965362332084 and parameters: {'alpha': 0.966810900242425, 'lr': 3.83255165792661e-05}. Best is trial 1 with value: 0.24859965362332084.\n","output_type":"stream"},{"name":"stdout","text":"[Optuna] => Validation loss: 0.2486\nOptuna tamamlandı! Best params:\n{'alpha': 0.966810900242425, 'lr': 3.83255165792661e-05}\nBest value (val_loss): 0.24859965362332084\n\n=== Nihai eğitim başlıyor ===\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-2-b26102738f1a>:292: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n","output_type":"stream"},{"name":"stdout","text":"Nihai eğitim: 2 GPU var, DataParallel aktif.\n[Nihai] => Epoch 1/25 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"NihaiTrain E1:   0%|          | 0/274 [00:00<?, ?batch/s]<ipython-input-2-b26102738f1a>:305: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nNihaiTrain E1: 100%|██████████| 274/274 [06:45<00:00,  1.48s/batch, loss=0.6808]\n","output_type":"stream"},{"name":"stdout","text":"[Nihai] => Epoch 1 bitti. Loss: 0.6808, Süre: 405.28 sn, LR: [3.817835515585664e-05]\n[Nihai] => Epoch 2/25 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"NihaiTrain E2: 100%|██████████| 274/274 [06:47<00:00,  1.49s/batch, loss=0.3309]\n","output_type":"stream"},{"name":"stdout","text":"[Nihai] => Epoch 2 bitti. Loss: 0.3309, Süre: 407.72 sn, LR: [3.77391917091854e-05]\n[Nihai] => Epoch 3/25 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"NihaiTrain E3: 100%|██████████| 274/274 [06:44<00:00,  1.48s/batch, loss=0.2094]\n","output_type":"stream"},{"name":"stdout","text":"[Nihai] => Epoch 3 bitti. Loss: 0.2094, Süre: 404.63 sn, LR: [3.7014952109149905e-05]\n[Nihai] => Epoch 4/25 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"NihaiTrain E4: 100%|██████████| 274/274 [06:44<00:00,  1.47s/batch, loss=0.1559]\n","output_type":"stream"},{"name":"stdout","text":"[Nihai] => Epoch 4 bitti. Loss: 0.1559, Süre: 404.01 sn, LR: [3.6017058046882484e-05]\n[Nihai] => Epoch 5/25 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"NihaiTrain E5: 100%|██████████| 274/274 [06:48<00:00,  1.49s/batch, loss=0.1244]\n","output_type":"stream"},{"name":"stdout","text":"[Nihai] => Epoch 5 bitti. Loss: 0.1244, Süre: 408.67 sn, LR: [3.476124690785812e-05]\n[Nihai] => Epoch 6/25 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"NihaiTrain E6: 100%|██████████| 274/274 [06:44<00:00,  1.48s/batch, loss=0.1096]\n","output_type":"stream"},{"name":"stdout","text":"[Nihai] => Epoch 6 bitti. Loss: 0.1096, Süre: 404.81 sn, LR: [3.326732358392443e-05]\n[Nihai] => Epoch 7/25 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"NihaiTrain E7: 100%|██████████| 274/274 [06:57<00:00,  1.52s/batch, loss=0.1026]\n","output_type":"stream"},{"name":"stdout","text":"[Nihai] => Epoch 7 bitti. Loss: 0.1026, Süre: 417.10 sn, LR: [3.155884813832639e-05]\n[Nihai] => Epoch 8/25 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"NihaiTrain E8: 100%|██████████| 274/274 [06:47<00:00,  1.49s/batch, loss=0.0934]\n","output_type":"stream"},{"name":"stdout","text":"[Nihai] => Epoch 8 bitti. Loss: 0.0934, Süre: 407.60 sn, LR: [2.9662764249434833e-05]\n[Nihai] => Epoch 9/25 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"NihaiTrain E9: 100%|██████████| 274/274 [06:48<00:00,  1.49s/batch, loss=0.0874]\n","output_type":"stream"},{"name":"stdout","text":"[Nihai] => Epoch 9 bitti. Loss: 0.0874, Süre: 408.03 sn, LR: [2.76089742928432e-05]\n[Nihai] => Epoch 10/25 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"NihaiTrain E10: 100%|██████████| 274/274 [06:48<00:00,  1.49s/batch, loss=0.0819]\n","output_type":"stream"},{"name":"stdout","text":"[Nihai] => Epoch 10 bitti. Loss: 0.0819, Süre: 408.65 sn, LR: [2.5429867763041592e-05]\n[Nihai] => Epoch 11/25 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"NihaiTrain E11: 100%|██████████| 274/274 [06:49<00:00,  1.49s/batch, loss=0.0780]\n","output_type":"stream"},{"name":"stdout","text":"[Nihai] => Epoch 11 bitti. Loss: 0.0780, Süre: 409.21 sn, LR: [2.3159810471740128e-05]\n[Nihai] => Epoch 12/25 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"NihaiTrain E12: 100%|██████████| 274/274 [06:49<00:00,  1.50s/batch, loss=0.0777]\n","output_type":"stream"},{"name":"stdout","text":"[Nihai] => Epoch 12 bitti. Loss: 0.0777, Süre: 409.88 sn, LR: [2.0834602578489114e-05]\n[Nihai] => Epoch 13/25 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"NihaiTrain E13: 100%|██████████| 274/274 [06:50<00:00,  1.50s/batch, loss=0.0741]\n","output_type":"stream"},{"name":"stdout","text":"[Nihai] => Epoch 13 bitti. Loss: 0.0741, Süre: 410.60 sn, LR: [1.8490914000776995e-05]\n[Nihai] => Epoch 14/25 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"NihaiTrain E14: 100%|██████████| 274/274 [06:46<00:00,  1.48s/batch, loss=0.0695]\n","output_type":"stream"},{"name":"stdout","text":"[Nihai] => Epoch 14 bitti. Loss: 0.0695, Süre: 406.75 sn, LR: [1.616570610752598e-05]\n[Nihai] => Epoch 15/25 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"NihaiTrain E15: 100%|██████████| 274/274 [06:50<00:00,  1.50s/batch, loss=0.0740]\n","output_type":"stream"},{"name":"stdout","text":"[Nihai] => Epoch 15 bitti. Loss: 0.0740, Süre: 410.90 sn, LR: [1.3895648816224517e-05]\n[Nihai] => Epoch 16/25 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"NihaiTrain E16: 100%|██████████| 274/274 [06:47<00:00,  1.49s/batch, loss=0.0682]\n","output_type":"stream"},{"name":"stdout","text":"[Nihai] => Epoch 16 bitti. Loss: 0.0682, Süre: 407.86 sn, LR: [1.1716542286422902e-05]\n[Nihai] => Epoch 17/25 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"NihaiTrain E17: 100%|██████████| 274/274 [06:48<00:00,  1.49s/batch, loss=0.0653]\n","output_type":"stream"},{"name":"stdout","text":"[Nihai] => Epoch 17 bitti. Loss: 0.0653, Süre: 408.97 sn, LR: [9.662752329831267e-06]\n[Nihai] => Epoch 18/25 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"NihaiTrain E18: 100%|██████████| 274/274 [06:47<00:00,  1.49s/batch, loss=0.0636]\n","output_type":"stream"},{"name":"stdout","text":"[Nihai] => Epoch 18 bitti. Loss: 0.0636, Süre: 407.33 sn, LR: [7.766668440939718e-06]\n[Nihai] => Epoch 19/25 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"NihaiTrain E19: 100%|██████████| 274/274 [06:44<00:00,  1.48s/batch, loss=0.0647]\n","output_type":"stream"},{"name":"stdout","text":"[Nihai] => Epoch 19 bitti. Loss: 0.0647, Süre: 404.84 sn, LR: [6.0581929953416795e-06]\n[Nihai] => Epoch 20/25 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"NihaiTrain E20: 100%|██████████| 274/274 [06:46<00:00,  1.49s/batch, loss=0.0635]\n","output_type":"stream"},{"name":"stdout","text":"[Nihai] => Epoch 20 bitti. Loss: 0.0635, Süre: 406.94 sn, LR: [4.564269671407987e-06]\n[Nihai] => Epoch 21/25 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"NihaiTrain E21: 100%|██████████| 274/274 [06:49<00:00,  1.49s/batch, loss=0.0629]\n","output_type":"stream"},{"name":"stdout","text":"[Nihai] => Epoch 21 bitti. Loss: 0.0629, Süre: 409.08 sn, LR: [3.3084585323836184e-06]\n[Nihai] => Epoch 22/25 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"NihaiTrain E22: 100%|██████████| 274/274 [06:45<00:00,  1.48s/batch, loss=0.0624]\n","output_type":"stream"},{"name":"stdout","text":"[Nihai] => Epoch 22 bitti. Loss: 0.0624, Süre: 405.44 sn, LR: [2.3105644701162007e-06]\n[Nihai] => Epoch 23/25 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"NihaiTrain E23: 100%|██████████| 274/274 [06:45<00:00,  1.48s/batch, loss=0.0616]\n","output_type":"stream"},{"name":"stdout","text":"[Nihai] => Epoch 23 bitti. Loss: 0.0616, Süre: 405.46 sn, LR: [1.5863248700807082e-06]\n[Nihai] => Epoch 24/25 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"NihaiTrain E24: 100%|██████████| 274/274 [06:47<00:00,  1.49s/batch, loss=0.0597]\n","output_type":"stream"},{"name":"stdout","text":"[Nihai] => Epoch 24 bitti. Loss: 0.0597, Süre: 407.54 sn, LR: [1.1471614234094626e-06]\n[Nihai] => Epoch 25/25 başlıyor...\n","output_type":"stream"},{"name":"stderr","text":"NihaiTrain E25: 100%|██████████| 274/274 [06:42<00:00,  1.47s/batch, loss=0.0593]\n","output_type":"stream"},{"name":"stdout","text":"[Nihai] => Epoch 25 bitti. Loss: 0.0593, Süre: 402.93 sn, LR: [1e-06]\nNihai eğitim tamamlandı.\nModel kaydedildi: /kaggle/working/final_model.pth\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\n\n# Kaydedilmiş checkpoint yükle\nsaved_state_dict = torch.load(\"final_model.pth\", map_location=\"cpu\")\n\n# Yeni sözlük oluşturup 'module.' prefixini ayıkla\nnew_state_dict = {}\nfor k, v in saved_state_dict.items():\n    if k.startswith(\"module.\"):\n        new_key = k[len(\"module.\"):]  # 'module.' uzunluğu kadar kes\n    else:\n        new_key = k\n    new_state_dict[new_key] = v\n\n# Şimdi new_state_dict parametre adları 'module.' olmadan\ntorch.save(new_state_dict, \"final_model_no_module.pth\")\nprint(\"Yeni checkpoint kaydedildi: final_model_no_module.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T06:43:42.491644Z","iopub.execute_input":"2024-12-22T06:43:42.492148Z","iopub.status.idle":"2024-12-22T06:43:43.156108Z","shell.execute_reply.started":"2024-12-22T06:43:42.492119Z","shell.execute_reply":"2024-12-22T06:43:43.155008Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-3-07db80e46eb1>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  saved_state_dict = torch.load(\"final_model.pth\", map_location=\"cpu\")\n","output_type":"stream"},{"name":"stdout","text":"Yeni checkpoint kaydedildi: final_model_no_module.pth\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"##########################################\n# TEST / INFERENCE KODU (ÖZET)\n##########################################\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoModel, AutoConfig\nfrom torchvision import transforms\nimport pandas as pd\nfrom PIL import Image\nimport numpy as np\nfrom tqdm import tqdm\n\n# 1) Z-Score değerleri, inverse fonksiyon vs.\nLAT_MEAN = 41.105763\nLAT_STD  = 0.002358\nLON_MEAN = 29.025191\nLON_STD  = 0.004205\ndef inverse_zscore_lat_lon(lat_norm, lon_norm):\n    lat_real = lat_norm * LAT_STD + LAT_MEAN\n    lon_real = lon_norm * LON_STD + LON_MEAN\n    return lat_real, lon_real\n\n# 2) TestDataset\nclass TestDataset(Dataset):\n    def __init__(self, dataframe, img_dir, transform=None):\n        self.dataframe = dataframe\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        image_path = os.path.join(self.img_dir, row[\"filename\"])\n        image = Image.open(image_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        return image\n\n\nTEST_CSV = \"/kaggle/input/hulyais/test.csv\"\nTEST_DIR = \"/kaggle/input/hulyais/test\"\n\ntest_df = pd.read_csv(TEST_CSV, sep=';')\n\ntest_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                         [0.229, 0.224, 0.225])\n])\ntest_dataset = TestDataset(test_df, TEST_DIR, transform=test_transform)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Model oluştur\nmodel = TransformerRegressor(\"microsoft/swin-base-patch4-window7-224\", output_dim=2)\nmodel.load_state_dict(torch.load(\"/kaggle/working/final_model_no_module.pth\", map_location=device))\n\n# 2 GPU istersek (opsiyonel)\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n\nmodel.to(device)\nmodel.eval()\n\npredictions_lat = []\npredictions_lon = []\n\nwith torch.no_grad():\n    for images in tqdm(test_loader, desc=\"Inference\", unit=\"batch\"):\n        images = images.to(device)\n        outputs = model(images)  # (batch_size, 2)\n        lat_norm = outputs[:, 0]\n        lon_norm = outputs[:, 1]\n        lat_vals, lon_vals = inverse_zscore_lat_lon(lat_norm, lon_norm)\n        lat_vals = lat_vals.cpu().numpy()\n        lon_vals = lon_vals.cpu().numpy()\n\n        predictions_lat.extend(lat_vals)\n        predictions_lon.extend(lon_vals)\n\ntest_df[\"latitude\"] = predictions_lat\ntest_df[\"longitude\"] = predictions_lon\n\nout_path = \"/kaggle/working/test_predictions.csv\"\ntest_df.to_csv(out_path, index=False)\nprint(f\"Tahminler kaydedildi: {out_path}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T06:44:20.018914Z","iopub.execute_input":"2024-12-22T06:44:20.019198Z","iopub.status.idle":"2024-12-22T06:47:46.929771Z","shell.execute_reply.started":"2024-12-22T06:44:20.019177Z","shell.execute_reply":"2024-12-22T06:47:46.929024Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-4-7380b393d2d9>:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"/kaggle/working/final_model_no_module.pth\", map_location=device))\nInference: 100%|██████████| 25/25 [03:25<00:00,  8.23s/batch]","output_type":"stream"},{"name":"stdout","text":"Tahminler kaydedildi: /kaggle/working/test_predictions.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4}]}